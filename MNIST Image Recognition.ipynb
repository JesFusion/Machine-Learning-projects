{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My first Deep Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data_set, mnist_image_info = tfds.load(name = 'mnist', with_info = True, as_supervised = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, testing_data = image_data_set['train'], image_data_set['test']\n",
    "\n",
    "# extracting our validation samples from the training samples (as a percentage of the training samples)\n",
    "\n",
    "number_of_validation_samples = 0.1 * mnist_image_info.splits['train'].num_examples\n",
    "\n",
    "integer_validation_samples = tf.cast(number_of_validation_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting number of test samples to integer (as float would give an error)\n",
    "\n",
    "number_of_test_samples = mnist_image_info.splits['test'].num_examples\n",
    "\n",
    "\n",
    "integer_test_samples = tf.cast(number_of_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scaling our input data\n",
    "\n",
    "# we'll create a function for that...\n",
    "def scaling_algorithm(input_image, label):\n",
    "    output_image = tf.cast(input_image, tf.float32)\n",
    "\n",
    "    # dividing each input by 255 to scale them between 0 and 1\n",
    "    output_image /= 255\n",
    "\n",
    "    # releasing our output...\n",
    "    return output_image, label\n",
    "\n",
    "scaled_train_and_validation_dataset = training_data.map(scaling_algorithm)\n",
    "\n",
    "scaled_test_dataset = testing_data.map(scaling_algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to shoffle our data, to make it completely random, because it is possible that the computer arranged them in an ascending order, which is very dangerous for our SGD optimizer, and will make it confused\n",
    "buffer_size_for_shuffling = 12985  # i don't like giving it definite numbers like 10000 or 12000. i like to make it random\n",
    "\n",
    "shuffled_TandV_dataset = scaled_train_and_validation_dataset.shuffle(buffer_size_for_shuffling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we need to extract our validation samples from the entire training and validation dataset\n",
    "\n",
    "validation_dataset = shuffled_TandV_dataset.take(integer_validation_samples) # note that integer_validation_samples is a number\n",
    "\n",
    "training_dataset = shuffled_TandV_dataset.skip(integer_validation_samples) # our training dataset becomes everything that's not on the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll divide our training data into batches\n",
    "\n",
    "size_per_batch = 100  # 100 samples per batch\n",
    "\n",
    "training_dataset = training_dataset.batch(size_per_batch) # the training data is being splitted into batches based on the specified number of samples per batch\n",
    "\n",
    "validation_dataset = validation_dataset.batch(integer_validation_samples)  # the validation data only has one batch, which is the number of samples in the validation data, this is because the validation dataset is only going to be used for forward propagation, as our model is using it to check whether it's doing well or not. it won't encounter back propagation, because the model won't train on it\n",
    "\n",
    "testing_dataset = scaled_test_dataset.batch(integer_test_samples) # the validation data only has one batch, which is the number of samples in the validation data, this is because the validation dataset is only going to be used for forward propagation, as our model is using it to test it's performance. it won't encounter back propagation, because the model won't train on it\n",
    "\n",
    "\n",
    "# now we will iterate the validation data\n",
    "\n",
    "validation_input_group, validation_targets_group = next(iter(validation_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>There'll be 4 layers:<br>An input layer<br>2 hidden layers<br>An output layer</p>\n",
    "<p>You can adjust them later if you want</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "model_algorithm"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model_input_size = 784  # 28 x 28 pixlels all stacked into one vector which is 784\n",
    "\n",
    "# our model output will be a number from 0 to 9\n",
    "model_output_size = 10\n",
    "\n",
    "hidden_layers_neuron_size = 213\n",
    "\n",
    "# each image has a dimension of 28 by 28 by 1\n",
    "tensor_input_shape = (28, 28, 1)\n",
    "\n",
    "activation_functions = ['relu', 'softmax', 'sigmoid', 'tanh', 'leaky_relu']\n",
    "\n",
    "# defining our model structure...\n",
    "image_recognition_algorithm = tf.keras.Sequential([\n",
    "    # we need to flatten our input tensor to one vector, thus we will use the \"Flatten\" method\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape = tensor_input_shape), # this is our input that has been flattened\n",
    "\n",
    "    # now, we'll use the \"Dense\" method that will compute the dot product of the inputs and weights and add the biases\n",
    "\n",
    "    # output = (inputs * weights) + biases\n",
    "    tf.keras.layers.Dense(hidden_layers_neuron_size, activation = activation_functions[0]), # this is our first hidden layer\n",
    "\n",
    "    tf.keras.layers.Dense(hidden_layers_neuron_size, activation = activation_functions[3]), # this is our second hidden layer\n",
    "\n",
    "    # third layer...\n",
    "    tf.keras.layers.Dense(hidden_layers_neuron_size, activation = activation_functions[2]),\n",
    "\n",
    "    # now, unto our output layer...\n",
    "    # we'll use \"softmax\" as the activation function for our output layer\n",
    "    tf.keras.layers.Dense(model_output_size, activation = activation_functions[1])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting our optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "model_compilation"
    ]
   },
   "outputs": [],
   "source": [
    "custom_optimizers = [tf.keras.optimizers.Adagrad(learning_rate = 0.0001), tf.keras.optimizers.Adagrad(learning_rate = 0.02), 'adam']\n",
    "\n",
    "image_recognition_algorithm.compile(optimizer = custom_optimizers[2], loss = 'sparse_categorical_crossentropy', metrics = ['accuracy']) # here, we can use different optimizers, \"sparse_categorical_crossentropy\" as our loss function, and \"accuracy\" as one of our models parameters we want to view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "540/540 - 10s - 19ms/step - accuracy: 0.9124 - loss: 0.3058 - val_accuracy: 0.9602 - val_loss: 0.1293\n",
      "Epoch 2/6\n",
      "540/540 - 5s - 9ms/step - accuracy: 0.9692 - loss: 0.1032 - val_accuracy: 0.9732 - val_loss: 0.0877\n",
      "Epoch 3/6\n",
      "540/540 - 5s - 9ms/step - accuracy: 0.9793 - loss: 0.0683 - val_accuracy: 0.9818 - val_loss: 0.0561\n",
      "Epoch 4/6\n",
      "540/540 - 4s - 8ms/step - accuracy: 0.9849 - loss: 0.0493 - val_accuracy: 0.9862 - val_loss: 0.0435\n",
      "Epoch 5/6\n",
      "540/540 - 5s - 9ms/step - accuracy: 0.9874 - loss: 0.0399 - val_accuracy: 0.9875 - val_loss: 0.0387\n",
      "Epoch 6/6\n",
      "540/540 - 5s - 9ms/step - accuracy: 0.9904 - loss: 0.0307 - val_accuracy: 0.9840 - val_loss: 0.0466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2740f0a7740>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, let's train the model we've worked hard to build...\n",
    "# we'll specify the number of epochs we want our model to go through\n",
    "\n",
    "specified_number_of_epochs = 6\n",
    "\n",
    "image_recognition_algorithm.fit(training_dataset, epochs = specified_number_of_epochs, validation_data = (validation_input_group, validation_targets_group), verbose = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
